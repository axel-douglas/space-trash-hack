"""Active learning helpers to prioritise the next recipes to validate.

The module operates on the candidate payloads generated by
``app.modules.generator.generate_candidates``.  The helpers score each
candidate based on uncertainty or expected improvement to aid operators when
selecting the next physical or simulated experiment.
"""

from __future__ import annotations

from dataclasses import dataclass
from statistics import mean
from typing import Any, Iterable, Mapping, MutableMapping, Sequence

TARGET_COLUMNS = ("rigidez", "estanqueidad", "energy_kwh", "water_l", "crew_min")


@dataclass(slots=True)
class Acquisition:
    """Representation of a candidate selected for active learning."""

    recipe_id: str
    process_id: str
    priority: float
    strategy: str
    metrics: MutableMapping[str, float]
    candidate: Mapping[str, Any]


def _as_mapping(payload: Any) -> Mapping[str, Any]:
    if isinstance(payload, Mapping):
        return payload
    if hasattr(payload, "_asdict"):
        try:
            return payload._asdict()  # type: ignore[attr-defined]
        except Exception:  # pragma: no cover - defensive
            return {}
    if hasattr(payload, "__dict__"):
        return getattr(payload, "__dict__")
    return {}


def _extract_confidence(candidate: Mapping[str, Any]) -> Mapping[str, tuple[float, float]]:
    payload = candidate.get("confidence_interval")
    if isinstance(payload, Mapping):
        try:
            return {
                str(name): (float(bounds[0]), float(bounds[1]))
                for name, bounds in payload.items()
            }
        except Exception:  # pragma: no cover - defensive
            return {}

    props = candidate.get("props")
    if props is not None and hasattr(props, "confidence_interval"):
        mapping = getattr(props, "confidence_interval")
        if isinstance(mapping, Mapping):
            return {
                str(name): (float(bounds[0]), float(bounds[1]))
                for name, bounds in mapping.items()
            }

    features = candidate.get("features")
    if isinstance(features, Mapping):
        mapping = features.get("confidence_interval")
        if isinstance(mapping, Mapping):
            try:
                return {
                    str(name): (float(bounds[0]), float(bounds[1]))
                    for name, bounds in mapping.items()
                }
            except Exception:  # pragma: no cover - defensive
                return {}

    return {}


def _extract_uncertainty(candidate: Mapping[str, Any]) -> Mapping[str, float]:
    payload = candidate.get("uncertainty")
    if isinstance(payload, Mapping):
        try:
            return {str(name): float(value) for name, value in payload.items()}
        except Exception:  # pragma: no cover - defensive
            return {}

    props = candidate.get("props")
    if props is not None and hasattr(props, "uncertainty"):
        mapping = getattr(props, "uncertainty")
        if isinstance(mapping, Mapping):
            return {str(name): float(value) for name, value in mapping.items()}

    features = candidate.get("features")
    if isinstance(features, Mapping):
        mapping = features.get("uncertainty")
        if isinstance(mapping, Mapping):
            try:
                return {str(name): float(value) for name, value in mapping.items()}
            except Exception:  # pragma: no cover - defensive
                return {}

    return {}


def _confidence_width(bounds: tuple[float, float]) -> float:
    low, high = bounds
    width = float(high) - float(low)
    return max(width, 0.0)


def _uncertainty_score(candidate: Mapping[str, Any], targets: Iterable[str]) -> float:
    targets = list(targets)
    ci_map = _extract_confidence(candidate)
    widths = [_confidence_width(ci_map[name]) for name in targets if name in ci_map]

    if widths:
        return float(mean(widths))

    unc_map = _extract_uncertainty(candidate)
    values = [abs(float(unc_map[name])) for name in targets if name in unc_map]
    if values:
        return float(mean(values))

    return 0.0


def _best_score(observations: Sequence[Mapping[str, Any]] | None) -> float:
    if not observations:
        return 0.0

    best = None
    for obs in observations:
        try:
            score = float(obs.get("score", 0.0))
        except Exception:  # pragma: no cover - defensive
            continue
        if best is None or score > best:
            best = score

    return float(best) if best is not None else 0.0


def _candidate_score(candidate: Mapping[str, Any]) -> float:
    if "score" in candidate:
        try:
            return float(candidate["score"])
        except Exception:  # pragma: no cover - defensive
            return 0.0

    breakdown = candidate.get("score_breakdown")
    if isinstance(breakdown, Mapping) and "total" in breakdown:
        try:
            return float(breakdown["total"])
        except Exception:  # pragma: no cover - defensive
            return 0.0

    props = candidate.get("props")
    if props is not None and hasattr(props, "to_targets"):
        try:
            targets = props.to_targets()
            if isinstance(targets, Mapping) and "rigidez" in targets:
                return float(targets["rigidez"])
        except Exception:  # pragma: no cover - defensive
            return 0.0

    return 0.0


def _candidate_ids(candidate: Mapping[str, Any]) -> tuple[str, str]:
    recipe_id = None
    process_id = None

    features = candidate.get("features")
    if isinstance(features, Mapping):
        recipe_id = features.get("recipe_id")
        process_id = features.get("process_id")

    if recipe_id is None:
        recipe_id = candidate.get("recipe_id")
    if process_id is None:
        process_id = candidate.get("process_id")

    if recipe_id is None and hasattr(candidate.get("props"), "source"):
        recipe_id = getattr(candidate.get("props"), "source")

    return str(recipe_id or ""), str(process_id or "")


def suggest_next_candidates(
    candidates: Sequence[Mapping[str, Any]],
    *,
    strategy: str = "uncertainty",
    top_n: int = 5,
    observations: Sequence[Mapping[str, Any]] | None = None,
    targets: Iterable[str] = TARGET_COLUMNS,
    exploration_bonus: float = 0.1,
) -> list[Acquisition]:
    """Rank candidates according to the requested active learning strategy."""

    if not candidates:
        return []

    targets = list(targets)
    ranked: list[Acquisition] = []

    best_score = _best_score(observations)
    for candidate in candidates:
        recipe_id, process_id = _candidate_ids(candidate)
        metrics: MutableMapping[str, float] = {}

        if strategy == "uncertainty":
            priority = _uncertainty_score(candidate, targets)
            metrics["uncertainty"] = priority
        elif strategy == "expected_improvement":
            predicted = _candidate_score(candidate)
            uncertainty = _uncertainty_score(candidate, targets)
            improvement = max(0.0, predicted - best_score)
            priority = improvement + exploration_bonus * uncertainty
            metrics.update(
                {
                    "predicted_score": predicted,
                    "baseline_score": best_score,
                    "improvement": improvement,
                    "uncertainty": uncertainty,
                }
            )
        else:  # pragma: no cover - defensive against typos
            raise ValueError(f"Unknown active learning strategy: {strategy}")

        ranked.append(
            Acquisition(
                recipe_id=recipe_id,
                process_id=process_id,
                priority=float(priority),
                strategy=strategy,
                metrics=metrics,
                candidate=candidate,
            )
        )

    ranked.sort(key=lambda item: item.priority, reverse=True)
    return ranked[: max(top_n, 0)]


__all__ = ["Acquisition", "suggest_next_candidates", "TARGET_COLUMNS"]

