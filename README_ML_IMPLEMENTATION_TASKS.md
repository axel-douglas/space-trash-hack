# Rex-AI â€” Backlog de implementaciÃ³n para el pipeline predictivo

Este backlog transforma el plan tÃ©cnico en sprints accionables. Cada tarea se puede convertir en issue o ticket. Las dependencias indican el orden sugerido y quÃ© entregables alimentan a otros equipos (app, ciencia de materiales, operaciones).

## Leyenda

| Campo | DescripciÃ³n |
| --- | --- |
| Prioridad | ğŸ”´ crÃ­tico, ğŸŸ  alto, ğŸŸ¡ medio, ğŸŸ¢ deseable |
| Tipo | `data`, `ml`, `app`, `ops`, `infra` |
| Entregable | Artefacto, script o documentaciÃ³n que debe quedar en el repo o en almacenamiento externo |

---

## Sprint 0 â€” Preparativos (dÃ­a 0)

| ID | Tarea | Prioridad | Tipo | Dependencias | Entregable |
| --- | --- | --- | --- | --- | --- |
| S0.1 | Inventariar datasets existentes (`datasets/`, `data/`, `feedback/`) y mapear columnas a features/targets | ğŸ”´ | data | â€” | `docs/data_inventory.md` |
| S0.2 | Definir esquema Parquet unificado (`features.parquet`, `labels.parquet`) con `recipe_id`, `process_id`, `label_source`, `label_weight` | ğŸ”´ | data | S0.1 | `datasets/schema.yaml` |
| S0.3 | Configurar entorno (dependencias scikit-learn, xgboost, torch opcional) y verificar ejecuciÃ³n de `python -m app.modules.model_training --help` | ğŸ”´ | infra | â€” | Log de instalaciÃ³n + `requirements-lock.txt` opcional |
| S0.4 | Crear tablero de seguimiento (Notion/Jira) enlazando estas tareas e identificando owners | ğŸŸ  | ops | â€” | URL del tablero (ver `docs/CHANGELOG.md`) |

## Sprint 1 â€” Pipeline de datos + modelo base (dÃ­as 1â€“2)

| ID | Tarea | Prioridad | Tipo | Dependencias | Entregable |
| --- | --- | --- | --- | --- | --- |
| S1.1 | Implementar script ETL `scripts/build_gold_dataset.py` que combine fuentes y genere `data/gold/{features,labels}.parquet` | ğŸ”´ | data | S0.2 | Script + Parquet de muestra |
| S1.2 | Extender `compute_feature_vector()` para incluir fracciones de Ã³xidos, flags EVA/CTB/multilayer, Ã­ndices Trash-to-Gas y parÃ¡metros de proceso | ğŸ”´ | ml | S1.1 | PR con cambios + pruebas unitarias |
| S1.3 | AÃ±adir targets `rigidez`, `estanqueidad`, `energy_kwh`, `water_l`, `crew_min` en el pipeline de entrenamiento (RandomForest multisalida) | ğŸ”´ | ml | S1.1 | `data/models/rexai_regressor.joblib`, `metadata.json` |
| S1.4 | Calcular intervalos CI95 usando varianza entre Ã¡rboles + residuales; persistir en `metadata.json` | ğŸ”´ | ml | S1.3 | Campos `confidence_interval` por target |
| S1.5 | Registrar `feature_importances_` y logging por receta (`ml_prediction`, `uncertainty`, `label_source`) | ğŸŸ  | ml | S1.3 | ActualizaciÃ³n en `app/modules/model_registry.py` |
| S1.6 | Crear notebook `notebooks/validate_model.ipynb` con mÃ©tricas MAE/RMSE y cobertura | ğŸŸ  | ml | S1.3 | Notebook con grÃ¡ficos guardados |

## Sprint 1 â€” IntegraciÃ³n en la app (dÃ­a 2)

| ID | Tarea | Prioridad | Tipo | Dependencias | Entregable |
| --- | --- | --- | --- | --- | --- |
| S1.7 | Actualizar `ModelRegistry` para cargar `metadata.json` y mostrar CI + label_source en la UI | ğŸ”´ | app | S1.3, S1.4, S1.5 | PR en `app/` + capturas UI |
| S1.8 | Implementar selector de modo (`ml_mode` vs `heuristic_mode`) con fallback controlado | ğŸŸ  | app | S1.7 | PR + prueba manual documentada |
| S1.9 | Mostrar explicabilidad (top features) junto a cada receta candidata | ğŸŸ¡ | app | S1.7 | PR + captura |

## Sprint 2 â€” GeneraciÃ³n y ranking (dÃ­as 3â€“4)

| ID | Tarea | Prioridad | Tipo | Dependencias | Entregable |
| --- | --- | --- | --- | --- | --- |
| S2.1 | Implementar generador combinatorio de recetas (`scripts/generate_candidates.py`) con lÃ­mites de search space | ğŸ”´ | ml | S1.2 | Script + JSON de candidatos |
| S2.2 | Integrar scoring multiobjetivo configurable por usuario (weights + penalizaciones) | ğŸ”´ | ml | S2.1 | FunciÃ³n `score_recipe()` con tests |
| S2.3 | Clasificadores auxiliares para flags de seguridad/proceso (`passes_seal`, `process_risk`) | ğŸŸ  | ml | S1.1 | Modelos guardados + mÃ©tricas |
| S2.4 | Pipeline de ranking que filtre top 10â€“20 recetas con CI y explicabilidad | ğŸŸ  | ml | S2.2, S2.3 | `scripts/rank_candidates.py` |
| S2.5 | UI: mostrar tabla de candidatos con bandas de confianza y tags de riesgo | ğŸŸ  | app | S2.4 | PR + captura |

## Sprint 3 â€” Feedback y aprendizaje activo (dÃ­as 5â€“6)

| ID | Tarea | Prioridad | Tipo | Dependencias | Entregable |
| --- | --- | --- | --- | --- | --- |
| S3.1 | DiseÃ±ar formato `feedback/recipes.parquet` con campos `label_source`, `label_weight`, `conf_lo_*`, `conf_hi_*` | ğŸ”´ | data | S0.2 | Esquema + ejemplo |
| S3.2 | Script `scripts/ingest_feedback.py` para incorporar nuevos labels al gold dataset | ğŸ”´ | data | S3.1, S1.1 | Script + test |
| S3.3 | Automatizar re-entrenamiento (`python -m app.modules.retrain_from_feedback`) con cron/manual | ğŸŸ  | ml | S3.2 | Job o documentaciÃ³n |
| S3.4 | Implementar estrategia de active learning (uncertainty sampling / expected improvement) para sugerir prÃ³ximos ensayos | ğŸŸ¡ | ml | S2.4 | MÃ³dulo `app/modules/active_learning.py` |
| S3.5 | Registrar experimentos fÃ­sicos/simulador en `datasets/raw/experiments_*.parquet` con trazabilidad | ğŸŸ  | ops | S3.1 | GuÃ­a operativa |

## Sprint 4 â€” Extensiones opcionales (dÃ­as 7â€“10)

| ID | Tarea | Prioridad | Tipo | Dependencias | Entregable |
| --- | --- | --- | --- | --- | --- |
| S4.1 | Entrenar modelos XGBoost por target y comparar performance | ğŸŸ¡ | ml | S1.3 | Resultados en notebook + modelos |
| S4.2 | Autoencoder tabular para detecciÃ³n de duplicados en espacio latente | ğŸŸ¡ | ml | S1.2 | `autoencoder.pt`, documentaciÃ³n |
| S4.3 | OptimizaciÃ³n en espacio latente (Bayesian Optimization / grid adaptativo) | ğŸŸ¢ | ml | S4.2 | Script `optimize_latent.py` |
| S4.4 | CI/CD: workflow que empaquete modelos y publique release ZIP (`package_model_bundle`) | ğŸŸ¡ | infra | S1.3 | Archivo YAML de pipeline |

## MÃ©tricas de Ã©xito y checkpoints

1. **Cobertura de CI95 â‰¥ 85â€¯%** en validaciÃ³n holdout para `energy_kwh` y `water_l`.
2. **MAE relativo â‰¤ 15â€¯%** vs baseline heurÃ­stico en `crew_min`.
3. **Filtro automÃ¡tico â‰¥ 99â€¯%** de recetas inviables en pruebas masivas.
4. **Tiempo de entrenamiento reproducible** (< 10 min en hardware objetivo) registrado en `metadata.json`.
5. **Dashboard para el jurado** con comparativa heurÃ­stica vs IA antes/despuÃ©s de 10 labels nuevos.

## PrÃ³ximos pasos inmediatos

1. Asignar owners a tareas Sprint 0 y Sprint 1.
2. Crear issues en GitHub/Linear usando este backlog como plantilla.
3. Programar daily de 15 min para seguimiento durante el hackathon.
