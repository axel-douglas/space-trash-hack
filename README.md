# Space Trash Hack ‚Äî Streamlit Demo (2025)

Demo ligera que muestra la l√≥gica del "cerebro de reciclaje" para Marte:
1) Inventario de residuos (NASA non-metabolic waste, simplificado)
2) Dise√±o de objetivo (TargetSpec)
3) Generaci√≥n de recetas (combinaciones + proceso)
4) Resultados y trade-offs (Pareto, Sankey, m√©tricas)

## Ejecuci√≥n

La entrada recomendada para lanzar la demo es:

```bash
streamlit run app/Home.py
```

No se requieren variables de entorno adicionales para el arranque interactivo.
Sin embargo, pod√©s personalizar d√≥nde se buscan los artefactos de datos y
modelos exportando las siguientes variables antes de ejecutar cualquier
entrypoint:

- `REXAI_DATA_ROOT`: ra√≠z alternativa que reemplaza a `data/` como punto de
  partida para datasets curados, logs y archivos "gold". Se admite tanto rutas
  absolutas como relativas o con `~`; siempre se normalizan a un path absoluto.
- `REXAI_MODELS_DIR`: ubicaci√≥n expl√≠cita para los bundles de modelos. Si no se
  define, por defecto se usa `<DATA_ROOT>/models`, aprovechando el valor
  resultante de `REXAI_DATA_ROOT` cuando est√° presente.

> ‚öôÔ∏è **Bootstrap obligatorio:** antes de cualquier `import app.*`, cada entrypoint
> de Streamlit (incluyendo los m√≥dulos dentro de `app/pages/`) debe llamar a
> `ensure_streamlit_entrypoint(__file__)`:
>
> ```python
> from app.bootstrap import ensure_streamlit_entrypoint
>
> PROJECT_ROOT = ensure_streamlit_entrypoint(__file__)
> ```
>
> Esto fuerza la inclusi√≥n de la ra√≠z del repositorio en `sys.path` cuando se
> ejecutan archivos sueltos con `streamlit run` o `python app/...`, devuelve el
> directorio que contiene `app/__init__.py` y evita los `ModuleNotFoundError`
> al importar `app.*`.
>
> üß™ **Scripts y tests**: para utilidades CLI o fixtures de Pytest, usa
> `ensure_project_root(__file__)` en vez de manipular `sys.path` a mano:
>
> ```python
> from app.bootstrap import ensure_project_root
>
> PROJECT_ROOT = ensure_project_root(__file__)
> ```
>
> As√≠ mantenemos un √∫nico helper oficial para exponer el paquete `app` sin
> reintroducir hacks circulares.

El script `app/Home.py` centraliza la vista de *Mission Overview* y act√∫a como
√∫nico entrypoint interactivo, manteniendo alineada la pantalla principal con el
paso "Overview" de la navegaci√≥n multipaso.

## Modo Demo 3D en el Mars Control Center

La pesta√±a **"Modo Demo"** del panel marciano ahora incorpora una escena 3D
interactiva sincronizada con la simulaci√≥n log√≠stica:

- El modelo GLB de Marte (`app/static/models/24881_Mars_1_6792.glb`) se expone
  como asset est√°tico y se renderiza mediante un `ScenegraphLayer` de PyDeck.
- Cada c√°psula orbital adopta una √≥rbita animada cuya posici√≥n depende de su
  `eta_minutes`; la escala se incrementa a medida que se aproxima al nodo de
  destino.
- Eventos cr√≠ticos emitidos por `advance_timeline()` o por el guion demo se
  resaltan autom√°ticamente en √°mbar/rojo, con trayectorias m√°s gruesas para
  facilitar las presentaciones.

Para activar la vista, ejecut√° la simulaci√≥n desde la pesta√±a **Flight Radar**
y luego abr√≠ la subsecci√≥n *"Vista orbital 3D"* dentro de **Modo Demo**. La
escena se mantiene en sincron√≠a con los ticks manuales o autom√°ticos del
simulador y comparte los mismos datos de telemetr√≠a que impulsan el radar 2D.

## M√≥dulos principales

La refactorizaci√≥n de 2025 separ√≥ responsabilidades clave para mantener el
c√≥digo testeable y comprensible:

| M√≥dulo | Responsabilidades |
| --- | --- |
| `app/modules/data_sources.py` | Resuelve rutas dentro de `datasets/`, normaliza taxonom√≠as NASA y expone el bundle cacheado de features oficiales. |
| `app/modules/generator.py` | Mezcla residuos, selecciona procesos y construye candidatos junto con sus features listos para inferencia. |
| `app/modules/logging_utils.py` | Construye payloads JSON serializables y mantiene escritores Parquet rotados para los logs de inferencia. |

Los tests unitarios apuntan a estos l√≠mites para garantizar que cada m√≥dulo
permanezca enfocado en su dominio (ingesta, generaci√≥n y logging). El m√≥dulo
`generator` simplemente importa `official_features_bundle()` desde
`data_sources` y re-exporta `append_inference_log()` desde `logging_utils`, de
modo que las pruebas pueden interceptar la ingesta o el logging sin tocar la
arquitectura de ensamblado de candidatos.

### Semillas reproducibles del generador

El ensamblador de recetas ahora acepta una semilla expl√≠cita para repetir una
sesi√≥n completa de generaci√≥n. Pod√©s fijarla de tres maneras equivalentes:

- Cargar la app Streamlit y completar el campo **"Semilla (opcional)"** antes
  de presionar "Generar recomendaciones".
- Ejecutar el CLI `python scripts/generate_candidates.py --seed 1234` para
  persistir los candidatos deterministas en `data/candidates.json`.
- Definir la variable de entorno `REXAI_GENERATOR_SEED=1234` antes de invocar
  cualquier entrypoint. El par√°metro expl√≠cito siempre tiene prioridad sobre la
  variable de entorno.

La semilla inicializa tanto el RNG global como los RNG por tarea, de modo que
los candidatos producidos (scores, combinaciones de residuos y desempates del
optimizador) se mantengan iguales entre ejecuciones.

> Nota: los helpers legacy `app/modules/branding.py`, `app/modules/charts.py`
> y `app/modules/embeddings.py` fueron eliminados. La app se apoya en
> `app/modules/ui_blocks.py` para el layout y en los modelos entrenados para
> producir incrustaciones latentes reales.

## Requisitos
- Python 3.10+
- `pip install -r requirements.txt`

### Fase 0 ‚Äî Verificaciones antes de entrenar

1. **Validar ingestas y revisar errores**. Ejecuta las utilidades de `app.modules.data_pipeline`
   (o los tests asociados) para verificar que los CSV/Parquet crudos cumplen los modelos de
   datos. Cada incidencia se registra en `data/logs/ingestion.errors.jsonl`; inspecciona ese
   archivo antes de continuar para corregir filas inv√°lidas o faltantes.
2. **Generar artefactos m√≠nimos**. Ejecut√°
   `python scripts/build_gold_dataset.py --output-dir data/gold` para refrescar el
   dataset curado y, luego, `python -m app.modules.model_training --gold data/gold --append-logs "data/logs/feedback_*.parquet"`
   para producir el pipeline base y los metadatos indispensables en `data/models/`
   (por ejemplo `data/models/rexai_regressor.joblib` y `data/models/metadata.json`). El
   plan detallado de entrenamiento y variantes est√° descrito en
   [README_ML_GAMEPLAN.md](README_ML_GAMEPLAN.md).
3. **Confirmar el modo activo**. Si `data/models/rexai_regressor.joblib` no existe cuando se
   levanta la aplicaci√≥n, `ModelRegistry` lanza un bootstrap autom√°tico que entrena un RandomForest
   sint√©tico ligero y lo guarda en `data/models/`. Esto evita tener que versionar binarios para
   ejecutar la demo por primera vez. Mientras el bootstrap corre (o si fallara), la app recurre a
   las reglas `heuristic_props` para estimar rigidez, estanqueidad, energ√≠a, agua y minutos de crew.
   En cuanto el modelo y sus metadatos (`metadata.json`, clasificadores y `trained_on = "gold_v1"`)
   est√°n presentes en `data/models/`, `ModelRegistry.ready` arranca en `True` y las predicciones
   pasan a provenir del ensemble entrenado (RandomForest + ensambles opcionales) con intervalos de
   confianza y explicabilidad verificables v√≠a `python -m scripts.verify_model_ready`.

## Entrenar y generar artefactos de IA

El pipeline de entrenamiento consume datasets f√≠sicos/qu√≠micos alineados con
los documentos de NASA y UCF:

- `datasets/raw/nasa_waste_inventory.csv`: taxonom√≠a de residuos no-metab√≥licos
  (pouches, espumas, EVA/CTB, textiles, nitrilo, etc.).
- `datasets/raw/mgs1_composition.csv` / `mgs1_oxides.csv` / `mgs1_properties.csv`:
  composici√≥n mineral√≥gica y propiedades de MGS-1 (regolito de referencia).
- `datasets/raw/nasa_trash_to_gas.csv`: rendimiento de procesos Trash-to-Gas y
  Trash-to-Supply-Gas para Gateway/Mars.
- `datasets/raw/logistics_to_living.csv`: eficiencia de reuso de CTB (Logistics-2-Living).

`app/modules/model_training.py` construye combinaciones realistas mezclando esos
datasets con el cat√°logo de procesos y genera:

- Dataset procesado en `datasets/processed/rexai_training_dataset.parquet`.
- Corridas sint√©ticas utilizadas para depurar el pipeline en
  `datasets/processed/ml/synthetic_runs.parquet`.
- Dataset procesado en `data/processed/rexai_training_dataset.parquet`.
- Pipeline RandomForest multi-salida (`data/models/rexai_regressor.joblib`) con
  estimaci√≥n de incertidumbre (desv√≠o entre √°rboles + residuales de validaci√≥n).
- Ensemble de modelos de "wow effect":
  - `data/models/rexai_xgboost.joblib` (boosting por target).
  - `data/models/rexai_tabtransformer.pt` (transformer tabular ligero, opcional si PyTorch est√° disponible).
- Autoencoder tabular opcional (`data/models/rexai_autoencoder.pt`, requiere PyTorch). Las
  incrustaciones latentes expuestas en la app provienen exclusivamente de este
  autoencoder entrenado; ya no existe ning√∫n helper heur√≠stico para fabricarlas
  manualmente.
- Metadatos en `data/models/metadata.json` (features, targets, fecha, m√©tricas,
  importancias de features, residuales, paths de artefactos).

Para regenerar todos los artefactos (mezclando datasets simulados con el
corpus dorado y feedback humano cuando est√° disponible):

```bash
python -m app.modules.model_training --gold data/gold --append-logs "data/logs/feedback_*.parquet"
```

### Reentrenar desde feedback humano

Cada sesi√≥n de la tripulaci√≥n registrada en el panel "Feedback & Impact"
genera archivos `data/logs/feedback_*.parquet` con las correcciones aplicadas
al proceso (rigidez, estanqueidad, penalizaciones de energ√≠a/agua/crew). El
comando dedicado `retrain_from_feedback` consume esos logs, convierte las
se√±ales en targets supervisados y re-ejecuta el pipeline principal con
`--append-logs` ya configurado:

```bash
python -m app.modules.retrain_from_feedback
```

Opcionalmente pod√©s especificar rutas alternativas:

```bash
python -m app.modules.retrain_from_feedback \
  --gold data/gold \
  --features data/gold/features.parquet \
  --logs "data/logs/custom_feedback_*.parquet"
```

Al finalizar, `data/models/metadata.json` y `metadata_gold.json` se actualizan
con la nueva fecha (`trained_at`) y el label de procedencia (`trained_on`, por
ejemplo `hil_v1` o `hybrid_v2`). La pantalla principal de la app refleja la
√∫ltima fecha de reentrenamiento y la mezcla utilizada.

Como la instancia de `ModelRegistry` queda cacheada v√≠a `st.cache_resource`,
tras copiar artefactos nuevos sin reiniciar la app record√° invalidar esa cach√©.
Pod√©s exponer un bot√≥n admin en Streamlit que llame a
`app.modules.ml_models.get_model_registry().clear()` o ejecutar manualmente:

```bash
python -c "from app.modules.ml_models import get_model_registry; get_model_registry.clear()"
```

Luego de limpiar la cach√©, refresc√° la p√°gina para que la UI vuelva a cargar la
metadata y los pipelines reci√©n entrenados.

> Nota: la optimizaci√≥n bayesiana con Ax/BoTorch es opcional. El entorno Streamlit
> detecta autom√°ticamente si `ax-platform` y `botorch` est√°n instalados; en caso
> contrario utiliza el optimizador heur√≠stico integrado. Para habilitarla basta con
> `pip install ax-platform botorch` antes de ejecutar la app.

### Logs generados en runtime

Los m√≥dulos que escriben m√©tricas o telemetr√≠a crean sus rutas din√°micamente en
runtime. Por ejemplo, `app/modules/impact.py` y `app/modules/logging_utils.py`
invocan `LOGS_DIR.mkdir(parents=True, exist_ok=True)` antes de guardar archivos
Parquet/JSON. Gracias a esto no es necesario versionar `data/logs/`; la carpeta
se materializa autom√°ticamente cuando se ejecutan los tests o la app y se
elimina limpiando los artefactos generados. Para verificar este flujo, ejecut√°:

```bash
pytest tests/test_impact_logging.py
```

La prueba crea un directorio temporal, persiste entradas de impacto y feedback
utilizando los m√≥dulos anteriores y confirma que los Parquet aparecen sin
requerir un placeholder en Git.

Los binarios (`.joblib`, `.pt`, `.parquet`) permanecen ignorados por Git para
mantener el repo liviano. Cuando existen localmente, la app reemplaza las
predicciones heur√≠sticas por las del modelo Rex-AI (RandomForest + XGBoost +
TabTransformer), expone bandas de confianza 95%, importancias promedio y el
vector latente entrenado sobre mezclas MGS-1 + residuos NASA. El bootstrap
autom√°tico genera un modelo sint√©tico con la etiqueta `trained_on = "synthetic_v0_bootstrap"`,
que pod√©s reemplazar en cualquier momento por artefactos reales empaquetados con
`python -m scripts.package_model_bundle --output dist/rexai_model_bundle_gold_v1.zip` para
distribuir un bundle reproducible en el que `ModelRegistry.ready` queda en `True` desde el arranque.

## Distribuci√≥n de artefactos ML

Los modelos entrenados se empaquetan autom√°ticamente con:

```bash
python -m scripts.package_model_bundle --output dist/rexai_model_bundle_gold_v1.zip
```

El script genera un ZIP reproducible con todos los binarios
(`data/models/rexai_regressor.joblib`, clasificadores y ensambles opcionales) y
ambas versiones de metadata (`metadata.json` y `metadata_gold.json`). Ese
bundle debe subirse como Release Asset (o artifact de CI) bajo el nombre
`rexai_model_bundle_gold_v1.zip`.

Para reutilizarlo en otro entorno:

```bash
wget https://github.com/<org>/<repo>/releases/latest/download/rexai_model_bundle_gold_v1.zip
unzip rexai_model_bundle_gold_v1.zip -d /tmp/rexai-models
rsync -av /tmp/rexai-models/data/models/ data/models/
```

Reemplaz√° `<org>/<repo>` por la organizaci√≥n y el nombre reales del repositorio.

Colocar los archivos dentro de `data/models/` **antes** de ejecutar
`streamlit run app/Home.py` garantiza que la app arranque directamente en modo
IA (`ready=True`) sin depender del bootstrap sint√©tico. El comando anterior deja
los archivos `.joblib` y `metadata.json` en el lugar correcto; si ya ten√©s el
ZIP generado localmente pod√©s simplemente descomprimirlo dentro del repositorio:

```bash
unzip dist/rexai_model_bundle_gold_v1.zip -d .
```

Esto recrear√° la estructura `data/models/` con los artefactos actualizados.
Antes de lanzar Streamlit ejecut√° `python -m scripts.verify_model_ready` para
confirmar que `ModelRegistry.ready` devuelve `True` y que la app usar√° el
ensemble entrenado desde el inicio.

### Actualizar el bundle publicado

1. **Entrenar**: `python -m app.modules.model_training --gold data/gold --append-logs "data/logs/feedback_*.parquet"`
   genera los `.joblib` y `metadata*.json` bajo `data/models/`.
2. **Verificar**: ejecut√° `python -m scripts.verify_model_ready` y conserv√° el
   JSON resultante como bit√°cora del reentrenamiento.
3. **Empaquetar**: `python -m scripts.package_model_bundle --output dist/rexai_model_bundle_<tag>.zip`
   produce el ZIP reproducible con todos los artefactos.
4. **Publicar**: sub√≠ el ZIP como release asset (o al almacenamiento acordado) y
   registr√° la URL final en `MODEL_BUNDLE_URL` junto con el hash en
   `MODEL_BUNDLE_SHA256` dentro de los secrets del despliegue. Actualiz√° la
   referencia cada vez que cambie `<tag>` para que la descarga autom√°tica apunte
   al bundle nuevo.

### Descarga autom√°tica desde secrets

Para despliegues donde no queremos subir los binarios al repositorio, la app
puede descargar el ZIP desde un release p√∫blico/privado antes de levantar el
registro de modelos. Configur√° los siguientes valores como variables de entorno
o en `st.secrets`:

```toml
# .streamlit/secrets.toml
MODEL_BUNDLE_URL = "https://github.com/<org>/<repo>/releases/download/v1.0.0/rexai_model_bundle_gold_v1.zip"
MODEL_BUNDLE_SHA256 = "<hash calculado con sha256sum>"
```

Con esas claves presentes, `ModelRegistry` descarga el bundle a un directorio
temporal, valida opcionalmente el hash y lo extrae autom√°ticamente sobre
`data/models/` antes de cargar `joblib`. Si los artefactos ya existen en el
directorio destino, la descarga se omite.

Flujo recomendado para publicar nuevos modelos:

1. Ejecut√° `python -m scripts.package_model_bundle --output dist/<bundle>.zip`.
2. Calcul√° el hash con `sha256sum dist/<bundle>.zip` (o `shasum -a 256`).
3. Carg√° el ZIP como Release Asset en GitHub y copi√° la URL de descarga directa
   (`https://github.com/<org>/<repo>/releases/download/...`).
4. Actualiz√° `MODEL_BUNDLE_URL` y `MODEL_BUNDLE_SHA256` en el entorno/secrets
   del despliegue (Streamlit Cloud, Hugging Face, etc.).

As√≠ cada reinicio de la app asegura que `data/models/` contenga la versi√≥n
publicada sin ejecutar el bootstrap sint√©tico.

### Mantener el bundle actualizado

1. Reentrena con los datasets m√°s recientes:

   ```bash
   python -m app.modules.model_training --gold data/gold --append-logs "data/logs/feedback_*.parquet"
   ```

   El pipeline actualizar√° `data/models/rexai_regressor.joblib`, clasificadores
   auxiliares, y escribir√° `data/models/metadata.json` con un `trained_on`
   legible por `ModelRegistry.trained_label()` (por ejemplo `hybrid_v1`).

2. Empaqueta los artefactos con `python -m scripts.package_model_bundle --output dist/rexai_model_bundle_hybrid_v1.zip`,
   verifica con `python -m scripts.verify_model_ready` y publica el ZIP
   resultante.

3. Documenta la fecha y el label (`trained_on`) en el release/changelog para que
   los despliegues confirmen qu√© dataset aliment√≥ el entrenamiento.

## Verificaci√≥n autom√°tica de readiness

Antes de publicar un release ejecutar:

```bash
python -m scripts.verify_model_ready
```

El chequeo carga `ModelRegistry`, valida que `ready == True`, que todas las
m√©tricas/residuales est√©n presentes en `metadata.json` y reporta las rutas de
artefactos generados. Si falta alg√∫n binario o metadata cr√≠tica, el script sale
con error para evitar releases inconsistentes.

## Benchmarks heur√≠sticos vs IA

Para auditar la deriva entre las reglas `heuristic_props` y el modelo Rex-AI
pod√©s ejecutar:

```bash
python scripts/run_benchmarks.py --format csv --with-ablation
```

El comando espera que `data/models/rexai_regressor.joblib` est√© disponible y
genera tablas comparativas en `data/benchmarks/`. Adem√°s de los archivos
`scenario_predictions.csv`/`scenario_metrics.csv`, la flag `--with-ablation`
a√±ade `ablation_predictions.csv` y `ablation_metrics.csv`, que documentan el
impacto de desactivar grupos de features (composici√≥n MGS-1, banderas NASA e
√≠ndices log√≠sticos) durante la inferencia.

Los errores se calculan tratando las heur√≠sticas como baseline. El resumen por
escenario (promediado sobre las cinco m√©tricas) incorpora tambi√©n el ancho
medio de los intervalos de confianza al 95% (`ci95_width_mean`):

| Escenario | MAE medio | RMSE | CI95 (ancho medio) |
|-----------|----------:|-----:|-------------------:|
| CTB Reconfig | 16‚ÄØ016 | 33‚ÄØ654 | 33‚ÄØ210 |
| Espuma + MGS-1 + Sinter | 16‚ÄØ090 | 34‚ÄØ298 | 31‚ÄØ687 |
| Multicapa + Laminar | 16‚ÄØ281 | 34‚ÄØ214 | 32‚ÄØ596 |
| Global (los 3 escenarios) | 16‚ÄØ129 | 34‚ÄØ056 | 32‚ÄØ498 |

Los valores anteriores provienen de `data/benchmarks/scenario_metrics.csv` y se
actualizan autom√°ticamente al rerunear el script.„ÄêF:data/benchmarks/scenario_metrics.csv‚Ä†L18-L25„Äë

### Observaciones

* **Gap frente a las heur√≠sticas**: el ensemble entrenado sobre `gold_v1` reduce
  el error medio global a 16‚ÄØ129 (RMSE 34‚ÄØ056), casi dos √≥rdenes de magnitud por
  debajo del bootstrap sint√©tico previo y alineado con las escalas de cada
  target.„ÄêF:data/benchmarks/scenario_metrics.csv‚Ä†L18-L25„Äë
* **Consumo de crew**: sigue siendo el m√°s sensible; el MAE por escenario ronda
  4‚ÄØ177 minutos, lo que equivale a una guardia extendida pero ya no a desv√≠os del
  orden de cientos de miles.„ÄêF:data/benchmarks/scenario_metrics.csv‚Ä†L18-L22„Äë
* **Energ√≠a y agua**: los errores promedio se estabilizan en ~76‚ÄØ032 kWh y
  435 litros agregados, consistentes con la variabilidad del dataset dorado.
  Las bandas de confianza capturan estos m√°rgenes (‚âà146‚ÄØ058 kWh y 937 L).„ÄêF:data/benchmarks/scenario_metrics.csv‚Ä†L21-L24„Äë
* **Rigidez/estanqueidad**: las discrepancias siguen acotadas (‚â§0.54), lo que
  facilita auditar calibraciones mec√°nicas sin perder precisi√≥n perceptiva.„ÄêF:data/benchmarks/scenario_predictions.csv‚Ä†L2-L23„Äë
* **CI95**: las bandas medias caen a ~32‚ÄØ498 unidades globales, con escenarios
  entre 31‚ÄØ687 y 33‚ÄØ210 gracias al reentrenamiento con el corpus dorado.„ÄêF:data/benchmarks/scenario_metrics.csv‚Ä†L18-L25„Äë

Consulta [BENCHMARK.md](BENCHMARK.md) para el resumen detallado, la metodolog√≠a
y c√≥mo interpretar los tres escenarios y los resultados de ablation. Con los
artefactos `gold_v1` cargados, el ensemble alcanza un MAE global de 16‚ÄØ129, RMSE
34‚ÄØ056 y bandas CI95 promedio de 32‚ÄØ498, demostrando una mejora dr√°stica frente
al bootstrap heur√≠stico original.„ÄêF:data/benchmarks/scenario_metrics.csv‚Ä†L18-L25„Äë
Los barridos con `--with-ablation` confirman ajustes pendientes: quitar las
banderas NASA o los √≠ndices log√≠sticos apenas modifica el MAE y puede ensanchar
las bandas de confianza, por lo que conviene priorizar calibraciones finas en
lugar de apagar features completos.„ÄêF:data/benchmarks/ablation_metrics.csv‚Ä†L71-L73„Äë

Despu√©s de incorporar feedback humano, ejecuta el benchmark nuevamente apuntando
a un directorio distinto (`--output-dir data/benchmarks/post_feedback`) y
genera un comparativo con:

```bash
python scripts/plot_benchmark_deltas.py \
  --before data/benchmarks/pre_feedback/scenario_metrics.csv \
  --after data/benchmarks/post_feedback/scenario_metrics.csv
```

El HTML resultante (`data/benchmarks/feedback_deltas.html`) grafica cu√°nto se
redujeron (o empeoraron) MAE, RMSE y el ancho del CI95 frente al baseline
anterior, evidenciando el impacto directo del feedback en las m√©tricas de la IA.

## Ejecutar la app

Con los artefactos generados, lanzar la demo con:

```bash
streamlit run app/Home.py
```

La UI detecta `ModelRegistry.ready` y, si los modelos est√°n presentes, muestra
predicciones, bandas de confianza, importancia de features y comparaciones con
los ensambles opcionales. Ante ausencia de modelos utiliza los fallbacks
heur√≠sticos originales.
